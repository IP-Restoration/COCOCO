# CoCoCo: Improving Text-Guided Video Inpainting for Better Consistency, Controllability and Compatibility

**[Bojia Zi1](https://scholar.google.fi/citations?user=QrMKIkEAAAAJ&hl=en), [Shihao Zhao^2](https://scholar.google.com/citations?user=dNQiLDQAAAAJ&hl=en), [Xianbiao Qi4](https://scholar.google.com/citations?user=odjSydQAAAAJ&hl=en), [Jianan Wang4](https://scholar.google.com/citations?user=mt5mvZ8AAAAJ&hl=en), [Yukai Shi3](https://scholar.google.com/citations?user=oQXfkSQAAAAJ&hl=en), [Qianyu Chen1](https://scholar.google.com/citations?user=Kh8FoLQAAAAJ&hl=en), [Bin Liang1](https://scholar.google.com/citations?user=djpQeLEAAAAJ&hl=en), [Kam-Fai Wong1](https://scholar.google.com/citations?user=fyMni2cAAAAJ&hl=en), [Lei Zhang4](https://scholar.google.com/citations?user=fIlGZToAAAAJ&hl=en)**

1 The Chinese University of Hong Kong   2 The University of Hong Kong   3 Tsinghua University   4 International Digital Economy Academy

This is the inference code for our paper CoCoCo


**The pretrained model is available [release](https://mycuhk-my.sharepoint.com/:f:/g/personal/1155203591_link_cuhk_edu_hk/EoXyViqDi8JEgBDCbxsyPY8BCg7YtkOy73SbBY-3WcQ72w?e=cDZuXM).**

**TO DO**
---------------------------------------
*We will use larger dataset with high-quality videos to produce a more powerful video inpainting model soon.*


